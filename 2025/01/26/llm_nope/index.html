<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLM实践2:大模型长度外推简单实践 | QianLi Yang</title><meta name="author" content="QianLi Yang"><meta name="copyright" content="QianLi Yang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="暑期实习的时候面试经常碰到面试官问如何拓展LLM的上下文，现在已经是标准的八股回答：Dynamic NTK，NTK Aware插值，YaRN等方法但是有的需要设计到微调，则比较麻烦，也有部分方法不需要微调，但是也没有系统性验证过可行性，现在闲来无事想利用空闲时间跑跑代码看看效果   具体的理论细节可以参考这篇博客：https:&#x2F;&#x2F;blog.csdn.net&#x2F;v_JULY_v&#x2F;article&#x2F;det">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM实践2:大模型长度外推简单实践">
<meta property="og:url" content="http://example.com/2025/01/26/llm_nope/index.html">
<meta property="og:site_name" content="QianLi Yang">
<meta property="og:description" content="暑期实习的时候面试经常碰到面试官问如何拓展LLM的上下文，现在已经是标准的八股回答：Dynamic NTK，NTK Aware插值，YaRN等方法但是有的需要设计到微调，则比较麻烦，也有部分方法不需要微调，但是也没有系统性验证过可行性，现在闲来无事想利用空闲时间跑跑代码看看效果   具体的理论细节可以参考这篇博客：https:&#x2F;&#x2F;blog.csdn.net&#x2F;v_JULY_v&#x2F;article&#x2F;det">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/head.jpg">
<meta property="article:published_time" content="2025-01-26T05:54:00.000Z">
<meta property="article:modified_time" content="2025-01-26T13:13:58.159Z">
<meta property="article:author" content="QianLi Yang">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/head.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/01/26/llm_nope/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM实践2:大模型长度外推简单实践',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/images/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://github.com/YangQianli92"><i class="fa-fw fab fa-github"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">QianLi Yang</span></a><a class="nav-page-title" href="/"><span class="site-name">LLM实践2:大模型长度外推简单实践</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://github.com/YangQianli92"><i class="fa-fw fab fa-github"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">LLM实践2:大模型长度外推简单实践</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-01-26T05:54:00.000Z" title="发表于 2025-01-26 13:54:00">2025-01-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-26T13:13:58.159Z" title="更新于 2025-01-26 21:13:58">2025-01-26</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id=""><a href="#" class="headerlink" title=""></a></h1><p>暑期实习的时候面试经常碰到面试官问如何拓展LLM的上下文，现在已经是标准的八股回答：Dynamic NTK，NTK Aware插值，YaRN等方法<br>但是有的需要设计到微调，则比较麻烦，也有部分方法不需要微调，但是也没有系统性验证过可行性，现在闲来无事想利用空闲时间跑跑代码看看效果  </p>
<p>具体的理论细节可以参考这篇博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/v_JULY_v/article/details/135072211">https://blog.csdn.net/v_JULY_v/article/details/135072211</a></p>
<p>本次实验采用<a target="_blank" rel="noopener" href="https://huggingface.co/meta-llama/Llama-2-7b-hf">LLaMA-2-7B</a>,<br>数据使用<a target="_blank" rel="noopener" href="https://www.modelscope.cn/datasets/BAAI/IndustryCorpus2/summary">BAAI</a>提供的数据，随机抽样了5000条进行预训练or推理</p>
<p>本实验将使用LLaMA_Factory&#x3D;&#x3D;0.9.1, transformers&#x3D;&#x3D;4.46.1,显卡为NVIDIA A100-PCIE-40GB * 2<br>max length&#x3D;8K，batch_size&#x3D;1,epoch&#x3D;1 GPU20GB ~ 30GB  </p>
<h3 id="实验结论"><a href="#实验结论" class="headerlink" title="实验结论"></a>实验结论</h3><table>
<thead>
<tr>
<th>RoPE Type</th>
<th>Max Length</th>
<th>Training Loss</th>
<th>Training Loss Image</th>
<th>Inference PPL</th>
</tr>
</thead>
<tbody><tr>
<td>None</td>
<td>8K</td>
<td>3.1112</td>
<td><img src="/images/training_loss.png" alt="Image"></td>
<td>13.7978</td>
</tr>
<tr>
<td>YaRN</td>
<td>8K</td>
<td>1.7079</td>
<td><img src="/images/training_loss_yarn.png" alt="Image"></td>
<td>5.64138</td>
</tr>
<tr>
<td>Dynamic</td>
<td>8K</td>
<td>3.1337</td>
<td><img src="/images/training_loss_dynamic.png" alt="Image"></td>
<td>5.5951</td>
</tr>
</tbody></table>
<h3 id="训练脚本"><a href="#训练脚本" class="headerlink" title="训练脚本"></a>训练脚本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### model</span></span><br><span class="line">model_name_or_path: /data/mxp/qianli/models/Llama-2-7b-hf</span><br><span class="line">trust_remote_code: <span class="literal">true</span></span><br><span class="line">rope_scaling: dynamic <span class="comment"># 这里似乎无法传参进去，得修改/data/mxp/qianli/models/Llama-2-7b-hf/config.json下的结构</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### method</span></span><br><span class="line">stage: pt</span><br><span class="line">do_train: <span class="literal">true</span></span><br><span class="line">finetuning_type: lora</span><br><span class="line">lora_rank: 8</span><br><span class="line">lora_target: all</span><br><span class="line"></span><br><span class="line"><span class="comment">### dataset</span></span><br><span class="line">dataset: baai_pt</span><br><span class="line">cutoff_len: 8192</span><br><span class="line">max_samples: 5000</span><br><span class="line">overwrite_cache: <span class="literal">true</span></span><br><span class="line">preprocessing_num_workers: 16</span><br><span class="line"></span><br><span class="line"><span class="comment">### output</span></span><br><span class="line">output_dir: saves/llama3-8b/lora/pretrain_dynamic</span><br><span class="line">logging_steps: 10</span><br><span class="line">save_steps: 500</span><br><span class="line">plot_loss: <span class="literal">true</span></span><br><span class="line">overwrite_output_dir: <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### train</span></span><br><span class="line">per_device_train_batch_size: 1</span><br><span class="line">gradient_accumulation_steps: 8</span><br><span class="line">learning_rate: 1.0e-4</span><br><span class="line">num_train_epochs: 1.0</span><br><span class="line">lr_scheduler_type: cosine</span><br><span class="line">warmup_ratio: 0.1</span><br><span class="line">bf16: <span class="literal">true</span></span><br><span class="line">ddp_timeout: 180000000</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>使用LLaMA_Factory的时候，似乎在bash脚本中修改rope_scaling是无效的，需要模型路径下的Config.json和generation.json中进行修改</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;_name_or_path&quot;</span>: <span class="string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>,</span><br><span class="line">  <span class="string">&quot;architectures&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;LlamaForCausalLM&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;bos_token_id&quot;</span>: 1,</span><br><span class="line">  <span class="string">&quot;eos_token_id&quot;</span>: 2,</span><br><span class="line">  <span class="string">&quot;hidden_act&quot;</span>: <span class="string">&quot;silu&quot;</span>,</span><br><span class="line">  <span class="string">&quot;hidden_size&quot;</span>: 4096,</span><br><span class="line">  <span class="string">&quot;initializer_range&quot;</span>: 0.02,</span><br><span class="line">  <span class="string">&quot;intermediate_size&quot;</span>: 11008,</span><br><span class="line">  <span class="string">&quot;max_position_embeddings&quot;</span>: 4096,</span><br><span class="line">  <span class="string">&quot;model_type&quot;</span>: <span class="string">&quot;llama&quot;</span>,</span><br><span class="line">  <span class="string">&quot;num_attention_heads&quot;</span>: 32,</span><br><span class="line">  <span class="string">&quot;num_hidden_layers&quot;</span>: 32,</span><br><span class="line">  <span class="string">&quot;num_key_value_heads&quot;</span>: 32,</span><br><span class="line">  <span class="string">&quot;pretraining_tp&quot;</span>: 1,</span><br><span class="line">  <span class="string">&quot;rms_norm_eps&quot;</span>: 1e-05,</span><br><span class="line">  <span class="string">&quot;rope_scaling&quot;</span>: null,</span><br><span class="line">  <span class="string">&quot;rope_type&quot;</span>: <span class="string">&quot;dynamic&quot;</span>,  <span class="comment"># 添加所需要的RoPE type</span></span><br><span class="line">  <span class="string">&quot;tie_word_embeddings&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">  <span class="string">&quot;torch_dtype&quot;</span>: <span class="string">&quot;float16&quot;</span>,</span><br><span class="line">  <span class="string">&quot;transformers_version&quot;</span>: <span class="string">&quot;4.31.0.dev0&quot;</span>,</span><br><span class="line">  <span class="string">&quot;use_cache&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">&quot;vocab_size&quot;</span>: 32000</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>同时需要修改对应的最长上下文，当然与cut_off好像也有点冲突，似乎<strong>模型自带的config优先级会大于bash中的</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;bos_token_id&quot;</span>: 1,</span><br><span class="line">  <span class="string">&quot;do_sample&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">&quot;eos_token_id&quot;</span>: 2,</span><br><span class="line">  <span class="string">&quot;pad_token_id&quot;</span>: 0,</span><br><span class="line">  <span class="string">&quot;temperature&quot;</span>: 0.6,</span><br><span class="line">  <span class="string">&quot;max_length&quot;</span>: 4096, <span class="comment"># 这里需要修改为8192</span></span><br><span class="line">  <span class="string">&quot;top_p&quot;</span>: 0.9,</span><br><span class="line">  <span class="string">&quot;transformers_version&quot;</span>: <span class="string">&quot;4.31.0.dev0&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="推理代码"><a href="#推理代码" class="headerlink" title="推理代码"></a>推理代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> LlamaTokenizer, LlamaForCausalLM,LlamaConfig</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&quot;/data/mxp/qianli/models/Llama-2-7b-hf&quot;</span></span><br><span class="line">device = <span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">config = LlamaConfig.from_pretrained(model_name)</span><br><span class="line"><span class="built_in">setattr</span>(config, <span class="string">&quot;rope_scaling&quot;</span>, &#123;</span><br><span class="line">    <span class="string">&quot;factor&quot;</span>: <span class="number">2.0</span>,</span><br><span class="line">    <span class="string">&quot;rope_type&quot;</span>: <span class="string">&quot;dynamic&quot;</span></span><br><span class="line">&#125;)</span><br><span class="line"><span class="built_in">setattr</span>(config, <span class="string">&quot;rope_theta&quot;</span>, <span class="number">10000</span>)</span><br><span class="line"><span class="built_in">setattr</span>(config, <span class="string">&quot;rope_type&quot;</span>, <span class="string">&quot;dynamic&quot;</span>)</span><br><span class="line"></span><br><span class="line">tokenizer = LlamaTokenizer.from_pretrained(model_name)</span><br><span class="line">model = LlamaForCausalLM.from_pretrained(model_name, config=config).to(device)</span><br><span class="line">model.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    <span class="keyword">if</span> param.data.dtype == torch.float32:</span><br><span class="line">        param.data = param.data.to(torch.float16)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> config.rope_scaling == <span class="string">&#x27;dynamic&#x27;</span>:</span><br><span class="line">    model.base_model.rotary_emb.original_inv_freq = model.base_model.rotary_emb.original_inv_freq.to(device)</span><br><span class="line"><span class="comment"># 函数：计算困惑度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_perplexity</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="comment"># 将文本编码为输入ID</span></span><br><span class="line">    inputs = tokenizer(text, return_tensors=<span class="string">&quot;pt&quot;</span>,truncation=<span class="literal">True</span>, max_length=<span class="number">8192</span>).to(device)</span><br><span class="line">    <span class="built_in">print</span>(inputs[<span class="string">&#x27;input_ids&#x27;</span>].size())</span><br><span class="line">    <span class="comment"># 获取模型输出的logits</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs, labels=inputs[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line">        log_likelihood = outputs.loss * inputs[<span class="string">&quot;input_ids&quot;</span>].shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算困惑度</span></span><br><span class="line">    perplexity = torch.exp(log_likelihood / inputs[<span class="string">&quot;input_ids&quot;</span>].shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> perplexity.isnan():</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line">    <span class="built_in">print</span>(perplexity.item())</span><br><span class="line">    <span class="keyword">return</span> perplexity.item()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载JSON文件</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_json_file</span>(<span class="params">file_path</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> json.load(f)[:<span class="number">5000</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：从JSON文件中提取文本并计算困惑度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_perplexity_from_json</span>(<span class="params">json_file_path</span>):</span><br><span class="line">    data = load_json_file(json_file_path)</span><br><span class="line"></span><br><span class="line">    total_ppl = <span class="number">0.0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> text <span class="keyword">in</span> tqdm(data):</span><br><span class="line">        ppl = calculate_perplexity(text[<span class="string">&#x27;text&#x27;</span>])</span><br><span class="line">        <span class="keyword">if</span> ppl != <span class="number">0.0</span>:</span><br><span class="line">            total_ppl += ppl</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Average ppl : <span class="subst">&#123;total_ppl / count&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 使用示例JSON路径</span></span><br><span class="line">json_file_path = <span class="string">&quot;data/baai_pt_film_0.json&quot;</span>  <span class="comment"># 替换为实际的JSON文件路径</span></span><br><span class="line">calculate_perplexity_from_json(json_file_path)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Dynamic-NTK插值实践"><a href="#Dynamic-NTK插值实践" class="headerlink" title="Dynamic-NTK插值实践"></a>Dynamic-NTK插值实践</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_47936614/article/details/141826966">Dynamic在Transformers中的代码详解</a></p>
<p>在transformers的LLamaRotraryEmbedding类中，存在一个bug（可能是我使用方法不对）  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LlamaRotaryEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        dim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        max_position_embeddings=<span class="number">2048</span>,</span></span><br><span class="line"><span class="params">        base=<span class="number">10000</span>,</span></span><br><span class="line"><span class="params">        device=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        scaling_factor=<span class="number">1.0</span>,</span></span><br><span class="line"><span class="params">        rope_type=<span class="string">&quot;default&quot;</span>,</span></span><br><span class="line"><span class="params">        config: <span class="type">Optional</span>[LlamaConfig] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># TODO (joao): remove the `if` below, only used for BC</span></span><br><span class="line">        <span class="variable language_">self</span>.rope_kwargs = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> config <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            logger.warning_once(</span><br><span class="line">                <span class="string">&quot;`LlamaRotaryEmbedding` can now be fully parameterized by passing the model config through the &quot;</span></span><br><span class="line">                <span class="string">&quot;`config` argument. All other arguments will be removed in v4.46&quot;</span></span><br><span class="line">            )</span><br><span class="line">            <span class="variable language_">self</span>.rope_kwargs = &#123;</span><br><span class="line">                <span class="string">&quot;rope_type&quot;</span>: rope_type,</span><br><span class="line">                <span class="string">&quot;factor&quot;</span>: scaling_factor,</span><br><span class="line">                <span class="string">&quot;dim&quot;</span>: dim,</span><br><span class="line">                <span class="string">&quot;base&quot;</span>: base,</span><br><span class="line">                <span class="string">&quot;max_position_embeddings&quot;</span>: max_position_embeddings,</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="variable language_">self</span>.rope_type = rope_type</span><br><span class="line">            <span class="variable language_">self</span>.max_seq_len_cached = max_position_embeddings</span><br><span class="line">            <span class="variable language_">self</span>.original_max_seq_len = max_position_embeddings</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># BC: &quot;rope_type&quot; was originally &quot;type&quot;</span></span><br><span class="line">            <span class="keyword">if</span> config.rope_scaling <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="variable language_">self</span>.rope_type = config.rope_scaling.get(<span class="string">&quot;rope_type&quot;</span>, config.rope_scaling.get(<span class="string">&quot;type&quot;</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="variable language_">self</span>.rope_type = <span class="string">&quot;default&quot;</span></span><br><span class="line">            <span class="variable language_">self</span>.max_seq_len_cached = config.max_position_embeddings</span><br><span class="line">            <span class="variable language_">self</span>.original_max_seq_len = config.max_position_embeddings</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.config = config</span><br><span class="line">        <span class="variable language_">self</span>.rope_init_fn = ROPE_INIT_FUNCTIONS[<span class="variable language_">self</span>.rope_type]</span><br><span class="line"></span><br><span class="line">        inv_freq, <span class="variable language_">self</span>.attention_scaling = <span class="variable language_">self</span>.rope_init_fn(<span class="variable language_">self</span>.config, device, **<span class="variable language_">self</span>.rope_kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq, persistent=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.original_inv_freq = <span class="variable language_">self</span>.inv_freq</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_dynamic_frequency_update</span>(<span class="params">self, position_ids, device</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        dynamic RoPE layers should recompute `inv_freq` in the following situations:</span></span><br><span class="line"><span class="string">        1 - growing beyond the cached sequence length (allow scaling)</span></span><br><span class="line"><span class="string">        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        seq_len = torch.<span class="built_in">max</span>(position_ids) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> seq_len &gt; <span class="variable language_">self</span>.max_seq_len_cached:  <span class="comment"># growth</span></span><br><span class="line">            inv_freq, <span class="variable language_">self</span>.attention_scaling = <span class="variable language_">self</span>.rope_init_fn(</span><br><span class="line">                <span class="variable language_">self</span>.config, device, seq_len=seq_len, **<span class="variable language_">self</span>.rope_kwargs</span><br><span class="line">            )</span><br><span class="line">            <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq, persistent=<span class="literal">False</span>)  <span class="comment"># TODO joao: may break with compilation</span></span><br><span class="line">            <span class="variable language_">self</span>.max_seq_len_cached = seq_len</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> seq_len &lt; <span class="variable language_">self</span>.original_max_seq_len <span class="keyword">and</span> <span class="variable language_">self</span>.max_seq_len_cached &gt; <span class="variable language_">self</span>.original_max_seq_len:  <span class="comment"># reset</span></span><br><span class="line">            <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, <span class="variable language_">self</span>.original_inv_freq, persistent=<span class="literal">False</span>)</span><br><span class="line">            <span class="variable language_">self</span>.max_seq_len_cached = <span class="variable language_">self</span>.original_max_seq_len</span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, position_ids</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;dynamic&quot;</span> <span class="keyword">in</span> <span class="variable language_">self</span>.rope_type:</span><br><span class="line">            <span class="variable language_">self</span>._dynamic_frequency_update(position_ids, device=x.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Core RoPE block</span></span><br><span class="line">        inv_freq_expanded = <span class="variable language_">self</span>.inv_freq[<span class="literal">None</span>, :, <span class="literal">None</span>].<span class="built_in">float</span>().expand(position_ids.shape[<span class="number">0</span>], -<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        position_ids_expanded = position_ids[:, <span class="literal">None</span>, :].<span class="built_in">float</span>()</span><br><span class="line">        <span class="comment"># Force float32 (see https://github.com/huggingface/transformers/pull/29285)</span></span><br><span class="line">        device_type = x.device.<span class="built_in">type</span></span><br><span class="line">        device_type = device_type <span class="keyword">if</span> <span class="built_in">isinstance</span>(device_type, <span class="built_in">str</span>) <span class="keyword">and</span> device_type != <span class="string">&quot;mps&quot;</span> <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">        <span class="keyword">with</span> torch.autocast(device_type=device_type, enabled=<span class="literal">False</span>):</span><br><span class="line">            freqs = (inv_freq_expanded.<span class="built_in">float</span>() @ position_ids_expanded.<span class="built_in">float</span>()).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>)</span><br><span class="line">            cos = emb.cos()</span><br><span class="line">            sin = emb.sin()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention</span></span><br><span class="line">        cos = cos * <span class="variable language_">self</span>.attention_scaling</span><br><span class="line">        sin = sin * <span class="variable language_">self</span>.attention_scaling</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>其中使用dynamic插值时，当input长度小于original_max_seq_len的时候，会出现如下错误</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument <span class="keyword">for</span> argument mat2 <span class="keyword">in</span> method wrapper_CUDA_bmm)</span><br></pre></td></tr></table></figure>
<p>经过debug会发现以下代码，其中错误时会有<code>inv_freq_expanded.devic=&#39;cpu&#39;</code>,回溯会发现时在该类初始化的时候， </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">```python</span><br><span class="line">if seq_len &lt; self.original_max_seq_len and self.max_seq_len_cached &gt; self.original_max_seq_len:  # reset</span><br><span class="line">    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span><br><span class="line">    self.max_seq_len_cached = self.original_max_seq_len</span><br><span class="line"></span><br><span class="line"># 省略一些不必要代码</span><br><span class="line">freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)</span><br></pre></td></tr></table></figure>

<p>因此解决方案就是在加载完model的时候，添加以下代码 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.base_model.rotary_emb.original_inv_freq = model.base_model.rotary_emb.original_inv_freq.to(device)</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">QianLi Yang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://yangqianli92.github.io/">https://yangqianli92.github.io/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">QianLi Yang</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/images/head.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/02/28/eign/" title="Equivariant Interaction-Aware Graph Network for Predicting the Binding Affinity of Protein-Ligand"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Equivariant Interaction-Aware Graph Network for Predicting the Binding Affinity of Protein-Ligand</div></div><div class="info-2"><div class="info-item-1">IntroductionThe success of drug discovery relies on predicting the binding affinity of protein-ligand. Applying deep learning to this field can expedite the process and reduce resource consumption. Recently, researchers have employed graph neural networks for predicting protein-ligand binding affinitiy, showcasing remarkable performance. However, this is largely attributed to the natural representation of biomolecule by graph neural networks, rather than a rational modeling of interactions...</div></div></div></a><a class="pagination-related" href="/2024/10/03/pbgpt/" title="PB-GPT:An innovative GPT-based model for protein backbone generation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">PB-GPT:An innovative GPT-based model for protein backbone generation</div></div><div class="info-2"><div class="info-item-1">IntroductionWith advanced computational methods, it is now feasible to modify or design proteins for specific functions, a process with significant implications for disease treatment and other medical applications. Protein structures and functions are intrinsically linked to their backbones, making the design of these backbones a pivotal aspect of protein engineering. In this study, we focus on the task of unconditionally generating protein backbones. By means of codebook quantization and...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/06/03/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A01/" title="LLM论文阅读-狼人杀系列 强化学习智能体在狼人杀中的策略玩法"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-03</div><div class="info-item-2">LLM论文阅读-狼人杀系列 强化学习智能体在狼人杀中的策略玩法</div></div><div class="info-2"><div class="info-item-1">#1 Language Agents with Reinforcement Learning for Strategic Play in the Werewolf GameAuthors: Zelai Xu, Chao Yu, Fei Fang, Yu Wang, Yi Wu Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model’s training data and results in suboptimal performance. To develop strategic language agents,...</div></div></div></a><a class="pagination-related" href="/2025/06/12/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A03/" title="LLM论文阅读-狼人杀系列 Werewolf 一个直观的游戏框架，配备文本转语音功能以提升用户参与度"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-12</div><div class="info-item-2">LLM论文阅读-狼人杀系列 Werewolf 一个直观的游戏框架，配备文本转语音功能以提升用户参与度</div></div><div class="info-2"><div class="info-item-1">Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement  Authors: Qihui Fan, Enfu Nan, Wenbo Li, Lei Lu, Pu Zhao, Yanzhi Wang The growing popularity of social deduction game systems for both business applications and AI research has greatly benefited from the rapid advancements in Large Language Models (LLMs), which now demonstrate stronger reasoning and persuasion capabilities. Especially with the raise of DeepSeek R1 and V3 models, LLMs should enable a more...</div></div></div></a><a class="pagination-related" href="/2025/06/10/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A02/" title="LLM论文阅读-狼人杀系列 探索LLM在社交类游戏的应用"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-10</div><div class="info-item-2">LLM论文阅读-狼人杀系列 探索LLM在社交类游戏的应用</div></div><div class="info-2"><div class="info-item-1">Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf  Authors: Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu Communication games, which we refer to as incomplete information games that heavily depend on natural language communication, hold significant research value in fields such as economics, social science, and artificial intelligence. In this work, we explore the problem of how to engage large language models (LLMs) in...</div></div></div></a><a class="pagination-related" href="/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A05/" title="LLM论文阅读-狼人杀系列5 Werewolf Arena"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-16</div><div class="info-item-2">LLM论文阅读-狼人杀系列5 Werewolf Arena</div></div><div class="info-2"><div class="info-item-1">Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction Authors: Suma Bailis, Jane Friedhoff, Feiyang Chen This paper introduces Werewolf Arena, a novel framework for evaluating large language models (LLMs) through the lens of the classic social deduction game, Werewolf. In Werewolf Arena, LLMs compete against each other, navigating the game’s complex dynamics of deception, deduction, and persuasion. The framework introduces a dynamic turn-taking system based on bidding,...</div></div></div></a><a class="pagination-related" href="/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A04/" title="LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-16</div><div class="info-item-2">LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型</div></div><div class="info-2"><div class="info-item-1">Training Language Models for Social Deduction with Multi-Agent Reinforcement LearningAuthors: Bidipta Sarkar, Warren Xia, C. Karen Liu, Dorsa Sadigh Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to...</div></div></div></a><a class="pagination-related" href="/2025/06/03/MCP-RAG/" title="LLM论文阅读:RAG-MCP：通过检索增强生成技术缓解大型语言模型工具选择中的提示膨胀问题"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-03</div><div class="info-item-2">LLM论文阅读:RAG-MCP：通过检索增强生成技术缓解大型语言模型工具选择中的提示膨胀问题</div></div><div class="info-2"><div class="info-item-1">RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation [PDF5] [Copy] [Kimi19] [REL]Authors: Tiantian Gan, Qiyao Sun Large language models (LLMs) struggle to effectively utilize a growing number of external tools, such as those defined by the Model Context Protocol (MCP)\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We introduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes this challenge by offloading tool...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/images/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">QianLi Yang</div><div class="author-info-description">不定时更新程序员成长之路~</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/YangQianli92" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:chainllie92@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/Serein_Young" target="_blank" title="Wechat"><i class="fab fa-weixin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text"></span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E8%AE%BA"><span class="toc-number">1.0.1.</span> <span class="toc-text">实验结论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC"><span class="toc-number">1.0.2.</span> <span class="toc-text">训练脚本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E4%BB%A3%E7%A0%81"><span class="toc-number">1.0.3.</span> <span class="toc-text">推理代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dynamic-NTK%E6%8F%92%E5%80%BC%E5%AE%9E%E8%B7%B5"><span class="toc-number">1.1.</span> <span class="toc-text">Dynamic-NTK插值实践</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/16/%E6%A2%A6%E5%B9%BB%E8%A5%BF%E6%B8%B8/" title="无标题">无标题</a><time datetime="2025-06-16T14:56:44.947Z" title="发表于 2025-06-16 22:56:44">2025-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/16/%E7%8B%BC%E4%BA%BA%E6%9D%80/" title="基于大语言模型（LLM）驱动的AI狼人杀综述">基于大语言模型（LLM）驱动的AI狼人杀综述</a><time datetime="2025-06-16T09:31:08.000Z" title="发表于 2025-06-16 17:31:08">2025-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A05/" title="LLM论文阅读-狼人杀系列5 Werewolf Arena">LLM论文阅读-狼人杀系列5 Werewolf Arena</a><time datetime="2025-06-16T04:46:48.000Z" title="发表于 2025-06-16 12:46:48">2025-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A04/" title="LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型">LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型</a><time datetime="2025-06-16T04:46:48.000Z" title="发表于 2025-06-16 12:46:48">2025-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/15/o1%E5%8E%9F%E7%90%86/" title="GPT-o1原理学习">GPT-o1原理学习</a><time datetime="2025-06-15T08:35:48.000Z" title="发表于 2025-06-15 16:35:48">2025-06-15</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By QianLi Yang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>