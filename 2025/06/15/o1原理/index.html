<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>GPT-o1原理学习 | QianLi Yang</title><meta name="author" content="QianLi Yang"><meta name="copyright" content="QianLi Yang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="GPT-o1原理OpenAI o1原理逆向工程图解的分析，O1的训练过程可归纳为以下阶段和核心机制：  一、预训练阶段（Pre-training） 数据配比调整O1的基座模型重新训练（非GPT-4o微调），大幅增加逻辑类数据（如STEM学科、代码、论文），减少通用知识数据，导致O1 mini“逻辑推理极强但世界知识弱”。 目标与传统LLM一致通过Next Token Prediction学习语言、">
<meta property="og:type" content="article">
<meta property="og:title" content="GPT-o1原理学习">
<meta property="og:url" content="http://example.com/2025/06/15/o1%E5%8E%9F%E7%90%86/index.html">
<meta property="og:site_name" content="QianLi Yang">
<meta property="og:description" content="GPT-o1原理OpenAI o1原理逆向工程图解的分析，O1的训练过程可归纳为以下阶段和核心机制：  一、预训练阶段（Pre-training） 数据配比调整O1的基座模型重新训练（非GPT-4o微调），大幅增加逻辑类数据（如STEM学科、代码、论文），减少通用知识数据，导致O1 mini“逻辑推理极强但世界知识弱”。 目标与传统LLM一致通过Next Token Prediction学习语言、">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/head.jpg">
<meta property="article:published_time" content="2025-06-15T08:35:48.000Z">
<meta property="article:modified_time" content="2025-06-15T14:58:27.077Z">
<meta property="article:author" content="QianLi Yang">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Paper">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/head.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/06/15/o1%E5%8E%9F%E7%90%86/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'GPT-o1原理学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/images/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://github.com/YangQianli92"><i class="fa-fw fab fa-github"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">QianLi Yang</span></a><a class="nav-page-title" href="/"><span class="site-name">GPT-o1原理学习</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://github.com/YangQianli92"><i class="fa-fw fab fa-github"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">GPT-o1原理学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-06-15T08:35:48.000Z" title="发表于 2025-06-15 16:35:48">2025-06-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-06-15T14:58:27.077Z" title="更新于 2025-06-15 22:58:27">2025-06-15</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="GPT-o1原理"><a href="#GPT-o1原理" class="headerlink" title="GPT-o1原理"></a>GPT-o1原理</h1><p>OpenAI o1原理逆向工程图解的分析，O1的训练过程可归纳为以下阶段和核心机制：</p>
<p><img src="/images/o1.png" alt="GPT-o1-R1模型训练流程"></p>
<h2 id="一、预训练阶段（Pre-training）"><a href="#一、预训练阶段（Pre-training）" class="headerlink" title="一、预训练阶段（Pre-training）"></a>一、预训练阶段（Pre-training）</h2><ol>
<li><strong>数据配比调整</strong><br>O1的基座模型重新训练（非GPT-4o微调），大幅增加逻辑类数据（如STEM学科、代码、论文），减少通用知识数据，导致O1 mini“逻辑推理极强但世界知识弱”。</li>
<li><strong>目标与传统LLM一致</strong><br>通过Next Token Prediction学习语言、基础推理等能力，但通过数据优化侧重逻辑内化。</li>
</ol>
<hr>
<h2 id="二、后训练阶段（Post-training）"><a href="#二、后训练阶段（Post-training）" class="headerlink" title="二、后训练阶段（Post-training）"></a>二、后训练阶段（Post-training）</h2><ol>
<li><p><strong>指令微调（SFT）</strong>  </p>
<p>使用逻辑推理类指令数据增强模型遵循指令的能力，为后续生成Hidden CoT铺垫。</p>
</li>
<li><p>Let’s Verify Step by Step技术继承</p>
<ul>
<li><p>数据风格：中间过程采用口语化、长文本、含反思纠错的结构（例：”wait a minute”&#x2F;“oops”），与PRM800k数据集风格高度一致</p>
</li>
<li><p>关键词特征：共享”alternatively”等标志性词汇，体现探索新路径的思维模式</p>
</li>
<li><p>标注规范：容忍非推进性步骤（标注为neutral而非negative），与o1冗余思考特征匹配</p>
</li>
<li><p>具体而言，该工作以<strong>GPT4-Base Model</strong>为基础，在MATH数据集上构造了中间推理数据，并以该数据集来微调Base Model，得到可以生成中间推理步骤的Generator； </p>
</li>
<li><p>采用人工标注，对Generator生成的每个step的1个或多个推理步骤进行标注，“-1”、“0”、“1”分别代表“negative”、“neutral”、“positive”； </p>
</li>
<li><p>基于标注的中间推理数据来训练process reward model；</p>
</li>
</ul>
</li>
<li><p><strong>安全对齐延迟处理</strong></p>
<p>传统RLHF的安全对齐未在此阶段完成，而是移至后续的“思考阶段”。</p>
</li>
</ol>
<hr>
<h2 id="三、强化学习（RL）训练阶段（关键创新，以下为猜测）"><a href="#三、强化学习（RL）训练阶段（关键创新，以下为猜测）" class="headerlink" title="三、强化学习（RL）训练阶段（关键创新，以下为猜测）"></a>三、强化学习（RL）训练阶段（关键创新，以下为猜测）</h2><ol>
<li><p><strong>训练目标</strong><br>通过RL训练模型生成<strong>Hidden CoT（隐藏思维链）</strong>，实现自我反思与错误修正能力。</p>
</li>
<li><p><strong>训练数据来源</strong>  </p>
<ul>
<li><strong>人工标注CoT</strong>：少量高质量&lt;问题，思考过程（含错误修正），答案&gt;数据启动训练。</li>
<li><strong>合成数据</strong>：从人工标注的COT里面截取一段人工标注片段，基于MCTS树搜索自动补全人工CoT片段，生成正&#x2F;负样本。对于有确定标准答案的逻辑问题，可以通过不断试错的模式直接从问题开始搜索正确答案，这里搜索到的正确答案和错误答案都可以用来训练o1模型</li>
<li><strong>反向生成数据</strong>：<ul>
<li><strong>代码领域</strong>：从现有代码反向生成推理步骤。</li>
<li><strong>数学领域</strong>：将AlphaProof的形式化解题过程转化为自然语言CoT。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>RL框架设计（参考AlphaZero）</strong>  </p>
<ul>
<li><strong>状态空间（State）</strong>：由Token序列组成的连续空间（类比RL游戏中的像素输入）。</li>
<li><strong>行为空间（Action）</strong>：定义离散的“思考因子”（如“拆解问题”“修正错误”），指导生成对应Token片段。</li>
<li><strong>奖励模型（Reward）</strong>：<ul>
<li><strong>ORM（结果奖励）</strong>：最终答案正确性奖励（稀疏但精准）。</li>
<li><strong>PRM（过程奖励）</strong>：对中间步骤评分（通过半自动标注生成训练数据）。</li>
</ul>
</li>
<li><strong>网络结构</strong>：  <ul>
<li>主LLM模型输出Token预测和RL策略。</li>
<li>策略网络（<em>P</em>）预测下一步“思考因子”，价值网络（<em>V</em>）评估当前状态通向正确答案的概率。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>MCTS树搜索整合</strong><br>在RL训练中引入树搜索（如控制宽度&#x2F;深度），通过策略和价值网络引导搜索路径，提升复杂问题解决能力。</p>
</li>
<li><p><strong>如何构造隐式 CoT 的优化过程的 Reward？</strong></p>
<p>可以通过不同温度采样出来的推理路径构建偏序，也可能是 MCTS 搜出来的正误参半的不同推理过程形成偏序。这点和先前的 MCTS 用法会有所不同，MCTS 节点上不再是最终生成答案中的某个 token 或某步，而是隐式推理过程中的每一步。 同时，为了提供更加细粒度的反馈和指导，需要引入过程性的奖励，而针对模型自身已经难以提供合理推理过程的复杂问题，通过引入额外的足够强的 Critic Model 来解决这个问题。最终通过强化学习，o1 学会了优化其思维链，并不断改进其使用的策略。它学会识别并纠正错误，学会将复杂的步骤分解为更简单的步骤，并在当前方法无效时尝试不同的解决方案。这个过程大幅提高了模型的推理能力</p>
</li>
</ol>
<hr>
<h2 id="四、推理阶段设计"><a href="#四、推理阶段设计" class="headerlink" title="四、推理阶段设计"></a>四、推理阶段设计</h2><ol>
<li><strong>三阶段流程</strong><br>用户提问 → 生成Hidden CoT → 提取CoT摘要 → 输出答案。</li>
<li><strong>安全机制实现</strong><br>在思考阶段后引入“AI宪法”式规则（如System Prompt中的安全条款），依赖增强后的逻辑能力自主遵循。</li>
<li><strong>模型架构</strong>  <ul>
<li><strong>主模型</strong>：生成Hidden CoT。</li>
<li><strong>摘要模型</strong>：压缩CoT为关键步骤。</li>
<li><strong>可扩展模型池</strong>：支持树搜索的多个子模型（数量可配置，影响效果和成本）。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="关键补充说明"><a href="#关键补充说明" class="headerlink" title="关键补充说明"></a>关键补充说明</h2><ol>
<li><strong>与传统LLM的核心差异</strong><br>O1通过RL内化“慢思考”能力，而非仅依赖推理时技巧（如Best-of-N Sampling）。</li>
<li><strong>成本与性能平衡</strong><br>输入价格高（如o1 mini为GPT-4o mini的20倍）源于多模型协作和树搜索算力需求。</li>
<li><strong>逆向工程局限性</strong><br>部分设计（如“思考因子”必要性）仍属推测，OpenAI未公开技术细节。</li>
</ol>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>本章节内容主要来源以下文章文章：<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/721952915">https://zhuanlan.zhihu.com/p/721952915</a> </p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/735117907">https://zhuanlan.zhihu.com/p/735117907</a></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/FXGdJA8OyZvLl89rXJiyAQ">https://mp.weixin.qq.com/s/FXGdJA8OyZvLl89rXJiyAQ</a></p>
<hr>
<h1 id="DeepSeek-R1-技术原理"><a href="#DeepSeek-R1-技术原理" class="headerlink" title="DeepSeek-R1 技术原理"></a>DeepSeek-R1 技术原理</h1><h2 id="训练流程概述"><a href="#训练流程概述" class="headerlink" title="训练流程概述"></a>训练流程概述</h2><p>DeepSeek-R1的训练分为三个阶段：</p>
<p>阶段一：以 DeepSeek-V3-Base（671B参数） 为起点，直接进行强化学习训练，得到 DeepSeek-R1-Zero。</p>
<p>阶段二：基于R1-Zero生成高质量监督数据（SFT数据），并整合非推理数据。</p>
<p>阶段三：用合成数据对基础模型微调（SFT），再进行强化学习，得到最终模型 DeepSeek-R1。</p>
<h2 id="核心训练步骤"><a href="#核心训练步骤" class="headerlink" title="核心训练步骤"></a>核心训练步骤</h2><p><img src="/images/ds-r1.png" alt="DeepSeek-R1模型训练流程"></p>
<ol>
<li><strong>DeepSeek-R1-Zero的训练（无SFT的纯RL）</strong></li>
</ol>
<p>方法：直接对预训练模型应用强化学习（Group Relative Policy Optimization, GRPO）。</p>
<p>奖励设计：</p>
<p>准确性奖励：针对数学&#x2F;代码问题，通过规则校验答案正确性（如LeetCode测试用例）。</p>
<p>格式奖励：强制模型将思考过程包裹在<thinking>和</thinking>标签中，确保输出结构化。</p>
<p>优势：在未使用监督数据的情况下，AIME 2024评测得分从15.6%提升至71.0%，推理能力显著增强</p>
<p>问题：输出可读性差，存在语言混用现象</p>
<ol start="2">
<li><strong>改进RL：加入SFT冷启动</strong></li>
</ol>
<p>优化点：为解决R1-Zero的问题，在RL前增加监督微调（SFT）。</p>
<p>SFT数据生成：</p>
<p>用R1-Zero生成少量（数千条）长链推理数据，人工校验后加入可读性模板：<br>|special_token|<reasoning_process>|special_token|<summary>。<br>RL奖励新增：加入语言一致性奖励（确保思考过程使用目标语言），提升人类友好性</p>
<p>(3) 高质量数据合成<br>推理数据（600k）：<br>使用改进版模型通过拒绝采样（rejection sampling） 生成，覆盖规则无法评估的复杂问题，并用<strong>DeepSeek-V3</strong>校验质量, 即通过输入的Ground truth和Prediction来进行检验，这一块可以看看llm as a judge相关的工作。</p>
<p>非推理数据（200k）：<br>从DeepSeek-V3的SFT数据中筛选（如写作、翻译、事实问答），部分数据添加CoT步骤</p>
<p>数据清洗：过滤语言混杂、含代码块的长段落</p>
<p>(4) 最终训练：SFT + RL<br>SFT微调：用800k合成数据（600k推理+200k非推理）对DeepSeek-V3-Base微调2轮，接下来就要进行RL了。</p>
<p>强化学习：</p>
<p>数学&#x2F;代码类任务：沿用规则奖励（准确性+格式）。</p>
<p>通用任务：使用DeepSeek-V3作为奖励模型，关注：</p>
<p>Helpfulness：评估最终回复的实用性。</p>
<p>Harmlessness：全程监控安全风险。</p>
<ol start="3">
<li><strong>关键技术亮点</strong></li>
</ol>
<p>纯RL的有效性：R1-Zero证明RL可自发提升复杂问题解决能力（如思考长度增至10k token）。</p>
<p>数据蒸馏价值：800k合成数据用于蒸馏Qwen&#x2F;Llama系列模型，效果超越原版（如Qwen-32B蒸馏版优于官方Preview）</p>
<p>失败尝试：</p>
<p>过程奖励模型（PRM）：因步骤划分模糊和奖励作弊问题被弃用。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08152"><strong>树搜索算法（如MCTS）：语言搜索空间过大，易陷入局部最优</strong></a></p>
<p>这里我个人也觉得，之前大家把重心放到放到MCTS上很奇怪，我觉得MCTS是在赌LLM一定具有正确答案，企图在搜索过程中找到。</p>
<ol start="4">
<li><strong>效果验证</strong><br>评测设置：上下文32k token，采样温度0.6，top-p 0.5。</li>
</ol>
<p>结果：DeepSeek-R1在数学推理（如AIME）、代码生成等任务达到SOTA，通用能力同步提升</p>
<h2 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/20062015354">https://zhuanlan.zhihu.com/p/20062015354</a></li>
<li>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, <a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2501.12948">https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2501.12948</a></li>
<li><a target="_blank" rel="noopener" href="https://x.com/SirrahChan/status/1881488738473357753?s=19">https://x.com/SirrahChan/status/1881488738473357753?s=19</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08152">https://arxiv.org/abs/2408.08152</a></li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">QianLi Yang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://yangqianli92.github.io/">https://yangqianli92.github.io/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">QianLi Yang</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/Paper/">Paper</a></div><div class="post-share"><div class="social-share" data-image="/images/head.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A04/" title="LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型</div></div><div class="info-2"><div class="info-item-1">Training Language Models for Social Deduction with Multi-Agent Reinforcement LearningAuthors: Bidipta Sarkar, Warren Xia, C. Karen Liu, Dorsa Sadigh Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to...</div></div></div></a><a class="pagination-related" href="/2025/06/12/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A03/" title="LLM论文阅读-狼人杀系列 Werewolf 一个直观的游戏框架，配备文本转语音功能以提升用户参与度"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">LLM论文阅读-狼人杀系列 Werewolf 一个直观的游戏框架，配备文本转语音功能以提升用户参与度</div></div><div class="info-2"><div class="info-item-1">Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement  Authors: Qihui Fan, Enfu Nan, Wenbo Li, Lei Lu, Pu Zhao, Yanzhi Wang The growing popularity of social deduction game systems for both business applications and AI research has greatly benefited from the rapid advancements in Large Language Models (LLMs), which now demonstrate stronger reasoning and persuasion capabilities. Especially with the raise of DeepSeek R1 and V3 models, LLMs should enable a more...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/06/03/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A01/" title="LLM论文阅读-狼人杀系列 强化学习智能体在狼人杀中的策略玩法"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-03</div><div class="info-item-2">LLM论文阅读-狼人杀系列 强化学习智能体在狼人杀中的策略玩法</div></div><div class="info-2"><div class="info-item-1">#1 Language Agents with Reinforcement Learning for Strategic Play in the Werewolf GameAuthors: Zelai Xu, Chao Yu, Fei Fang, Yu Wang, Yi Wu Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model’s training data and results in suboptimal performance. To develop strategic language agents,...</div></div></div></a><a class="pagination-related" href="/2025/06/12/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A03/" title="LLM论文阅读-狼人杀系列 Werewolf 一个直观的游戏框架，配备文本转语音功能以提升用户参与度"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-12</div><div class="info-item-2">LLM论文阅读-狼人杀系列 Werewolf 一个直观的游戏框架，配备文本转语音功能以提升用户参与度</div></div><div class="info-2"><div class="info-item-1">Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement  Authors: Qihui Fan, Enfu Nan, Wenbo Li, Lei Lu, Pu Zhao, Yanzhi Wang The growing popularity of social deduction game systems for both business applications and AI research has greatly benefited from the rapid advancements in Large Language Models (LLMs), which now demonstrate stronger reasoning and persuasion capabilities. Especially with the raise of DeepSeek R1 and V3 models, LLMs should enable a more...</div></div></div></a><a class="pagination-related" href="/2025/06/10/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A02/" title="LLM论文阅读-狼人杀系列 探索LLM在社交类游戏的应用"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-10</div><div class="info-item-2">LLM论文阅读-狼人杀系列 探索LLM在社交类游戏的应用</div></div><div class="info-2"><div class="info-item-1">Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf  Authors: Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu Communication games, which we refer to as incomplete information games that heavily depend on natural language communication, hold significant research value in fields such as economics, social science, and artificial intelligence. In this work, we explore the problem of how to engage large language models (LLMs) in...</div></div></div></a><a class="pagination-related" href="/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A05/" title="LLM论文阅读-狼人杀系列5 Werewolf Arena"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-16</div><div class="info-item-2">LLM论文阅读-狼人杀系列5 Werewolf Arena</div></div><div class="info-2"><div class="info-item-1">Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction Authors: Suma Bailis, Jane Friedhoff, Feiyang Chen This paper introduces Werewolf Arena, a novel framework for evaluating large language models (LLMs) through the lens of the classic social deduction game, Werewolf. In Werewolf Arena, LLMs compete against each other, navigating the game’s complex dynamics of deception, deduction, and persuasion. The framework introduces a dynamic turn-taking system based on bidding,...</div></div></div></a><a class="pagination-related" href="/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A04/" title="LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-16</div><div class="info-item-2">LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型</div></div><div class="info-2"><div class="info-item-1">Training Language Models for Social Deduction with Multi-Agent Reinforcement LearningAuthors: Bidipta Sarkar, Warren Xia, C. Karen Liu, Dorsa Sadigh Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to...</div></div></div></a><a class="pagination-related" href="/2025/06/03/MCP-RAG/" title="LLM论文阅读:RAG-MCP：通过检索增强生成技术缓解大型语言模型工具选择中的提示膨胀问题"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-03</div><div class="info-item-2">LLM论文阅读:RAG-MCP：通过检索增强生成技术缓解大型语言模型工具选择中的提示膨胀问题</div></div><div class="info-2"><div class="info-item-1">RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation [PDF5] [Copy] [Kimi19] [REL]Authors: Tiantian Gan, Qiyao Sun Large language models (LLMs) struggle to effectively utilize a growing number of external tools, such as those defined by the Model Context Protocol (MCP)\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We introduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes this challenge by offloading tool...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/images/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">QianLi Yang</div><div class="author-info-description">不定时更新程序员成长之路~</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/YangQianli92" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:chainllie92@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/Serein_Young" target="_blank" title="Wechat"><i class="fab fa-weixin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#GPT-o1%E5%8E%9F%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">GPT-o1原理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E9%A2%84%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5%EF%BC%88Pre-training%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">一、预训练阶段（Pre-training）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%90%8E%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5%EF%BC%88Post-training%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">二、后训练阶段（Post-training）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88RL%EF%BC%89%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5%EF%BC%88%E5%85%B3%E9%94%AE%E5%88%9B%E6%96%B0%EF%BC%8C%E4%BB%A5%E4%B8%8B%E4%B8%BA%E7%8C%9C%E6%B5%8B%EF%BC%89"><span class="toc-number">1.3.</span> <span class="toc-text">三、强化学习（RL）训练阶段（关键创新，以下为猜测）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.4.</span> <span class="toc-text">四、推理阶段设计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E"><span class="toc-number">1.5.</span> <span class="toc-text">关键补充说明</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-number">1.6.</span> <span class="toc-text">References</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DeepSeek-R1-%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">DeepSeek-R1 技术原理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E6%A6%82%E8%BF%B0"><span class="toc-number">2.1.</span> <span class="toc-text">训练流程概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E8%AE%AD%E7%BB%83%E6%AD%A5%E9%AA%A4"><span class="toc-number">2.2.</span> <span class="toc-text">核心训练步骤</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References-1"><span class="toc-number">2.3.</span> <span class="toc-text">References</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/16/%E6%A2%A6%E5%B9%BB%E8%A5%BF%E6%B8%B8/" title="无标题">无标题</a><time datetime="2025-06-16T14:56:44.947Z" title="发表于 2025-06-16 22:56:44">2025-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/16/%E7%8B%BC%E4%BA%BA%E6%9D%80/" title="基于大语言模型（LLM）驱动的AI狼人杀综述">基于大语言模型（LLM）驱动的AI狼人杀综述</a><time datetime="2025-06-16T09:31:08.000Z" title="发表于 2025-06-16 17:31:08">2025-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A05/" title="LLM论文阅读-狼人杀系列5 Werewolf Arena">LLM论文阅读-狼人杀系列5 Werewolf Arena</a><time datetime="2025-06-16T04:46:48.000Z" title="发表于 2025-06-16 12:46:48">2025-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A04/" title="LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型">LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型</a><time datetime="2025-06-16T04:46:48.000Z" title="发表于 2025-06-16 12:46:48">2025-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/15/o1%E5%8E%9F%E7%90%86/" title="GPT-o1原理学习">GPT-o1原理学习</a><time datetime="2025-06-15T08:35:48.000Z" title="发表于 2025-06-15 16:35:48">2025-06-15</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By QianLi Yang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>