<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型 | QianLi Yang</title><meta name="author" content="QianLi Yang"><meta name="copyright" content="QianLi Yang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Training Language Models for Social Deduction with Multi-Agent Reinforcement LearningAuthors: Bidipta Sarkar, Warren Xia, C. Karen Liu, Dorsa Sadigh Communicating in natural language is a powerful too">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型">
<meta property="og:url" content="http://example.com/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A04/index.html">
<meta property="og:site_name" content="QianLi Yang">
<meta property="og:description" content="Training Language Models for Social Deduction with Multi-Agent Reinforcement LearningAuthors: Bidipta Sarkar, Warren Xia, C. Karen Liu, Dorsa Sadigh Communicating in natural language is a powerful too">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/head.jpg">
<meta property="article:published_time" content="2025-06-16T04:46:48.000Z">
<meta property="article:modified_time" content="2025-06-16T14:04:43.817Z">
<meta property="article:author" content="QianLi Yang">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Paper">
<meta property="article:tag" content="werewolf">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/head.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A04/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/images/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">17</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://github.com/YangQianli92"><i class="fa-fw fab fa-github"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">QianLi Yang</span></a><a class="nav-page-title" href="/"><span class="site-name">LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://github.com/YangQianli92"><i class="fa-fw fab fa-github"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-06-16T04:46:48.000Z" title="发表于 2025-06-16 12:46:48">2025-06-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-06-16T14:04:43.817Z" title="更新于 2025-06-16 22:04:43">2025-06-16</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06060">Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning</a><br><strong>Authors</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/search/?searchtype=author&query=Bidipta%20Sarkar">Bidipta Sarkar</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/?searchtype=author&query=Warren%20Xia">Warren Xia</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/?searchtype=author&query=C.%20Karen%20Liu">C. Karen Liu</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/?searchtype=author&query=Dorsa%20Sadigh">Dorsa Sadigh</a></p>
<p>Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent’s goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model’s listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model’s speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on Among Us, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at <a target="_blank" rel="noopener" href="https://socialdeductionllm.github.io/">https://socialdeductionllm.github.io/</a></p>
<p><strong>Subjects</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/cs.AI">Artificial Intelligence</a> , <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/cs.CL">Computation and Language</a> , <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/cs.LG">Machine Learning</a> , <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/cs.MA">Multiagent Systems</a></p>
<p><strong>Publish</strong>: 2025-02-09 22:44:45 UTC</p>
<p><strong>Q</strong>: 这篇论文试图解决什么问题？</p>
<p><strong>A</strong>: 这篇论文试图解决在多智能体环境中，如何训练语言模型以实现有效自然语言沟通的问题，特别是在不需要大量人类演示数据的情况下。具体来说，论文关注于“社交推理”（social deduction）游戏场景，以《Among Us》游戏为研究对象，探索如何让智能体通过自然语言交流来推断游戏中的关键信息（例如谁是冒充者），从而提高团队合作和竞争的表现。</p>
<p>主要挑战包括：</p>
<ol>
<li><strong>缺乏人类演示数据</strong>：大多数现有方法依赖于大量人类交流数据来训练智能体，但在许多实际场景中，这些数据难以获取。</li>
<li><strong>稀疏奖励信号</strong>：在复杂的多智能体环境中，仅依靠游戏胜负等稀疏奖励信号来训练智能体是不够的，因为这无法提供足够的信息来指导智能体进行有效的沟通。</li>
<li><strong>智能体的沟通能力</strong>：需要让智能体学会如何“倾听”（理解其他智能体的信息）和“说话”（生成有用的信息），以促进团队合作和提高决策质量。</li>
</ol>
<p>为了解决这些问题，论文提出了一种新的训练方法，通过将智能体的目标（如推断冒充者的身份）转化为密集的奖励信号，来指导智能体的沟通行为。这种方法不仅提高了智能体的沟通能力，还使得智能体能够在没有人类演示的情况下自我改进其讨论策略。</p>
<p><strong>Q</strong>: 论文如何解决这个问题？</p>
<p><strong>A</strong>: 论文通过以下方法解决了在多智能体环境中训练语言模型进行有效自然语言沟通的问题：</p>
<h3 id="1-问题分解"><a href="#1-问题分解" class="headerlink" title="1. 问题分解"></a>1. <strong>问题分解</strong></h3><p>论文将沟通问题分解为两个部分：“倾听”（listening）和“说话”（speaking）。这种分解使得可以分别针对这两个方面进行优化，从而提高智能体的整体沟通能力。</p>
<h3 id="2-利用智能体的目标作为奖励信号"><a href="#2-利用智能体的目标作为奖励信号" class="headerlink" title="2. 利用智能体的目标作为奖励信号"></a>2. <strong>利用智能体的目标作为奖励信号</strong></h3><p>论文的核心思想是利用智能体的目标（例如，在《Among Us》中推断冒充者的身份）来提供一个密集的奖励信号，从而指导沟通。具体来说，通过以下两个主要机制来实现：</p>
<h4 id="（1）倾听：冒充者预测（Imposter-Prediction）"><a href="#（1）倾听：冒充者预测（Imposter-Prediction）" class="headerlink" title="（1）倾听：冒充者预测（Imposter Prediction）"></a>（1）倾听：冒充者预测（Imposter Prediction）</h4><ul>
<li><strong>方法</strong>：训练智能体根据环境观察和之前的讨论来预测冒充者的身份。这通过一个监督学习任务实现，其中智能体在每个讨论阶段被要求预测冒充者的身份，其预测的准确性作为奖励信号。</li>
<li><strong>作用</strong>：这使得智能体能够更好地理解其他智能体的信息，并据此更新自己的信念。通过这种方式，智能体学会了如何从讨论中提取有用信息。</li>
</ul>
<h4 id="（2）说话：强化讨论学习（Reinforced-Discussion-Learning）"><a href="#（2）说话：强化讨论学习（Reinforced-Discussion-Learning）" class="headerlink" title="（2）说话：强化讨论学习（Reinforced Discussion Learning）"></a>（2）说话：强化讨论学习（Reinforced Discussion Learning）</h4><ul>
<li><strong>方法</strong>：通过强化学习来训练智能体生成有用的讨论信息。具体来说，智能体的奖励基于其发言对其他智能体关于冒充者身份信念的影响。如果一个智能体的发言能够使其他智能体更接近正确的冒充者身份，那么这个发言就会得到正奖励。</li>
<li><strong>作用</strong>：这使得智能体能够学习如何生成有助于团队决策的讨论内容，从而提高整个团队的表现。</li>
</ul>
<h3 id="3-多智能体强化学习（Multi-Agent-Reinforcement-Learning-MARL）"><a href="#3-多智能体强化学习（Multi-Agent-Reinforcement-Learning-MARL）" class="headerlink" title="3. 多智能体强化学习（Multi-Agent Reinforcement Learning, MARL）"></a>3. <strong>多智能体强化学习（Multi-Agent Reinforcement Learning, MARL）</strong></h3><p>论文使用多智能体强化学习来训练智能体，使其能够在复杂的社交环境中进行有效的沟通和决策。通过让智能体在模拟环境中与对手（冒充者）和队友（其他船员）互动，智能体能够学习到如何在对抗性环境中进行有效的沟通和协调。</p>
<h3 id="4-迭代自我对弈（Iterated-Self-Play）"><a href="#4-迭代自我对弈（Iterated-Self-Play）" class="headerlink" title="4. 迭代自我对弈（Iterated Self-Play）"></a>4. <strong>迭代自我对弈（Iterated Self-Play）</strong></h3><p>为了使训练出的智能体能够应对不同策略的对手，论文采用了迭代自我对弈的方法。在每一轮迭代中，智能体与前一轮训练出的对手进行对弈，从而不断适应和改进自己的策略。这种方法使得智能体能够学习到如何应对不断进化的对手，从而提高其鲁棒性。</p>
<h3 id="5-使用大型语言模型（LLMs）"><a href="#5-使用大型语言模型（LLMs）" class="headerlink" title="5. 使用大型语言模型（LLMs）"></a>5. <strong>使用大型语言模型（LLMs）</strong></h3><p>论文利用了大型语言模型（如 RWKV）作为智能体的初始策略。这些模型由于其强大的语言生成能力，能够为智能体提供一个良好的起点。通过在上述框架中进一步训练这些模型，智能体能够自我改进其沟通能力，从而在复杂的社交推理游戏中表现出色。</p>
<h3 id="6-实验验证"><a href="#6-实验验证" class="headerlink" title="6. 实验验证"></a>6. <strong>实验验证</strong></h3><p>论文通过一系列实验验证了所提出方法的有效性。实验结果表明，经过训练的智能体在《Among Us》游戏中能够形成有效的沟通策略，其胜率显著高于仅使用强化学习或基础语言模型的智能体。此外，论文还分析了智能体在讨论阶段的新兴行为，如直接指责嫌疑人和提供证据，这些行为与人类玩家在实际游戏中的行为相似。</p>
<p>通过上述方法，论文成功地解决了在没有大量人类演示数据的情况下，如何训练智能体进行有效自然语言沟通的问题，并在社交推理游戏《Among Us》中展示了其有效性。</p>
<p><strong>Q</strong>: 论文做了哪些实验？</p>
<p><strong>A</strong>: 论文中进行了多个实验来验证所提出方法的有效性，这些实验主要集中在以下几个方面：</p>
<h3 id="1-不同训练算法的性能比较"><a href="#1-不同训练算法的性能比较" class="headerlink" title="1. 不同训练算法的性能比较"></a>1. <strong>不同训练算法的性能比较</strong></h3><p>论文首先比较了使用不同训练算法的智能体在《Among Us》游戏中的表现。具体来说，实验评估了以下几种算法：</p>
<ul>
<li><strong>基础语言模型（RWKV 和 RWKV7B）</strong>：直接使用预训练的语言模型作为智能体，没有额外训练。</li>
<li><strong>仅使用强化学习（RL）</strong>：仅使用游戏的稀疏奖励信号进行训练。</li>
<li><strong>仅使用倾听训练（L）</strong>：仅使用冒充者预测任务进行训练。</li>
<li><strong>强化学习加倾听（RL + L）</strong>：结合游戏的稀疏奖励信号和冒充者预测任务进行训练。</li>
<li><strong>强化学习加倾听加说话（RL + L + S）</strong>：结合游戏的稀疏奖励信号、冒充者预测任务以及讨论阶段的强化学习进行训练。</li>
</ul>
<p>实验结果表明，<strong>RL + L + S</strong> 算法的智能体在游戏中的表现最佳，其胜率是仅使用强化学习的智能体的两倍，且显著高于基础语言模型。</p>
<h3 id="2-环境配置的鲁棒性测试"><a href="#2-环境配置的鲁棒性测试" class="headerlink" title="2. 环境配置的鲁棒性测试"></a>2. <strong>环境配置的鲁棒性测试</strong></h3><p>为了测试智能体在不同环境配置下的鲁棒性，论文改变了游戏环境的多个参数，包括：</p>
<ul>
<li><strong>环境形状</strong>：改变房间的布局，如 2×1、1×3、2×2、2×3 和 3×2 的网格。</li>
<li><strong>任务数量</strong>：改变每个船员需要完成的任务数量，如 2、3、4 和 5 个任务。</li>
<li><strong>玩家数量</strong>：改变游戏中的玩家数量，如 4、5 和 6 名玩家。</li>
</ul>
<p>实验结果表明，使用 <strong>RL + L + S</strong> 算法训练的智能体在不同环境配置下均表现出良好的鲁棒性，其胜率在不同配置下保持稳定，且在训练分布之外的设置中也能有效外推其行为。</p>
<h3 id="3-对抗不同策略的冒充者的鲁棒性测试"><a href="#3-对抗不同策略的冒充者的鲁棒性测试" class="headerlink" title="3. 对抗不同策略的冒充者的鲁棒性测试"></a>3. <strong>对抗不同策略的冒充者的鲁棒性测试</strong></h3><p>为了评估智能体在对抗不同策略的冒充者时的鲁棒性，论文进行了多轮迭代自我对弈实验。在每一轮迭代中，船员智能体与前一轮训练出的冒充者智能体进行对弈，从而不断适应和改进自己的策略。</p>
<p>实验结果表明，经过几轮迭代自我对弈后，船员智能体的策略变得非常鲁棒，即使面对对抗性训练的冒充者，其胜率也能保持在较高水平（在基础环境中，胜率收敛到 0.51 到 0.56 之间）。</p>
<h3 id="4-讨论阶段的新兴行为分析"><a href="#4-讨论阶段的新兴行为分析" class="headerlink" title="4. 讨论阶段的新兴行为分析"></a>4. <strong>讨论阶段的新兴行为分析</strong></h3><p>论文还分析了智能体在讨论阶段的新兴行为。实验观察到，经过训练的智能体能够生成与人类玩家相似的讨论内容，例如：</p>
<ul>
<li><strong>直接指责嫌疑人</strong>：智能体会直接指出某个玩家是冒充者。</li>
<li><strong>提供证据</strong>：智能体会分享其观察到的环境信息，以支持其对冒充者的判断。</li>
</ul>
<p>这些新兴行为表明，智能体能够有效地利用讨论阶段来协调团队决策，从而提高团队的整体表现。</p>
<h3 id="5-失败模式分析"><a href="#5-失败模式分析" class="headerlink" title="5. 失败模式分析"></a>5. <strong>失败模式分析</strong></h3><p>论文还记录了在实验过程中遇到的一些失败模式，并探讨了如何通过调整训练算法来解决这些问题。例如：</p>
<ul>
<li><strong>语言退化</strong>：如果软 KL 约束设置不当，智能体可能会生成非自然语言的输出。</li>
<li><strong>社交规范退化</strong>：如果所有智能体同时训练，可能会出现一些简单的社交规范退化策略，如不说话或仅在起始房间等待。</li>
<li><strong>动作标记滥用</strong>：智能体可能会在讨论阶段使用动作标记，而不是生成自然语言。</li>
</ul>
<p>通过引入世界建模损失（world modeling loss）和冻结部分智能体等方法，论文成功地解决了这些失败模式，确保了智能体在讨论阶段能够生成高质量的自然语言内容。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过这些实验，论文验证了所提出方法在训练智能体进行有效自然语言沟通方面的有效性。实验结果表明，结合强化学习、冒充者预测任务和讨论阶段的强化学习的智能体在《Among Us》游戏中表现最佳，并且在不同环境配置和对抗性对手面前具有良好的鲁棒性。</p>
<p><strong>Q</strong>: 有什么可以进一步探索的点？</p>
<p><strong>A</strong>: 论文提出了一种在多智能体环境中训练语言模型进行有效自然语言沟通的方法，并在《Among Us》游戏中展示了其有效性。尽管取得了显著的成果，但仍有一些可以进一步探索的方向：</p>
<h3 id="1-更复杂的游戏环境"><a href="#1-更复杂的游戏环境" class="headerlink" title="1. 更复杂的游戏环境"></a>1. <strong>更复杂的游戏环境</strong></h3><ul>
<li><strong>扩展到更复杂的社交推理游戏</strong>：目前的研究集中在《Among Us》这一特定游戏中。可以探索其他更复杂的社交推理游戏，例如《Mafia》或《Secret Hitler》，这些游戏具有更复杂的规则和更多的玩家角色。</li>
<li><strong>动态环境</strong>：研究如何让智能体在动态变化的环境中进行有效沟通，例如环境中的任务和目标会随着时间变化。</li>
</ul>
<h3 id="2-更高级的语言模型"><a href="#2-更高级的语言模型" class="headerlink" title="2. 更高级的语言模型"></a>2. <strong>更高级的语言模型</strong></h3><ul>
<li><strong>使用更大规模的语言模型</strong>：虽然论文中使用了 1.5B 参数的 RWKV 模型，但可以探索更大规模的语言模型（如 GPT-4 或其他 100B 级别的模型）在复杂社交推理游戏中的表现。</li>
<li><strong>多模态输入</strong>：结合视觉、听觉等多模态输入，使智能体能够更全面地理解环境并进行更自然的沟通。</li>
</ul>
<h3 id="3-更复杂的沟通策略"><a href="#3-更复杂的沟通策略" class="headerlink" title="3. 更复杂的沟通策略"></a>3. <strong>更复杂的沟通策略</strong></h3><ul>
<li><strong>多轮对话管理</strong>：目前的研究主要集中在单轮对话的有效性。可以进一步研究如何让智能体在多轮对话中保持一致性和连贯性，以实现更复杂的沟通策略。</li>
<li><strong>情感和社交信号</strong>：探索如何让智能体在沟通中使用情感和社交信号，以更自然地与人类或其他智能体互动。</li>
</ul>
<h3 id="4-真实世界的任务"><a href="#4-真实世界的任务" class="headerlink" title="4. 真实世界的任务"></a>4. <strong>真实世界的任务</strong></h3><ul>
<li><strong>现实世界应用</strong>：将这种方法应用于现实世界中的任务，例如在微观尺度或有毒环境中部署合作智能体。研究如何让智能体在这些环境中进行有效的自然语言沟通。</li>
<li><strong>人机协作</strong>：研究如何让智能体与人类更自然地协作，特别是在需要精确沟通的任务中，例如医疗诊断、紧急响应等。</li>
</ul>
<h3 id="5-理论和算法改进"><a href="#5-理论和算法改进" class="headerlink" title="5. 理论和算法改进"></a>5. <strong>理论和算法改进</strong></h3><ul>
<li><strong>改进奖励信号设计</strong>：进一步研究如何设计更有效的奖励信号，以更好地指导智能体的沟通行为。</li>
<li><strong>多智能体学习算法</strong>：探索新的多智能体学习算法，以提高智能体在复杂环境中的学习效率和鲁棒性。</li>
</ul>
<h3 id="6-伦理和社会影响"><a href="#6-伦理和社会影响" class="headerlink" title="6. 伦理和社会影响"></a>6. <strong>伦理和社会影响</strong></h3><ul>
<li><strong>虚假信息和欺骗行为</strong>：研究如何减少智能体在沟通中生成虚假信息和欺骗行为，特别是在需要高度信任的场景中。</li>
<li><strong>社会影响分析</strong>：分析智能体的沟通行为对社会动态的影响，例如在团队合作、社区互动等场景中。</li>
</ul>
<h3 id="7-跨文化沟通"><a href="#7-跨文化沟通" class="headerlink" title="7. 跨文化沟通"></a>7. <strong>跨文化沟通</strong></h3><ul>
<li><strong>跨文化适应性</strong>：研究智能体如何适应不同文化背景下的沟通规范和策略，以实现更广泛的适用性。</li>
<li><strong>多语言环境</strong>：探索智能体在多语言环境中的沟通能力，特别是在需要跨语言协作的任务中。</li>
</ul>
<h3 id="8-长期记忆和知识积累"><a href="#8-长期记忆和知识积累" class="headerlink" title="8. 长期记忆和知识积累"></a>8. <strong>长期记忆和知识积累</strong></h3><ul>
<li><strong>长期记忆机制</strong>：研究如何为智能体设计长期记忆机制，使其能够在长时间跨度内积累和利用知识。</li>
<li><strong>知识更新和适应</strong>：探索智能体如何在环境变化时更新和适应其知识体系，以保持沟通的有效性。</li>
</ul>
<p>这些方向不仅可以进一步提升智能体在复杂社交推理游戏中的表现，还可以为多智能体系统在现实世界中的应用提供新的思路和方法。</p>
<p><strong>Q</strong>: 总结一下论文的主要内容</p>
<p><strong>A</strong>: 本文的核心内容是提出了一种在多智能体环境中训练语言模型进行有效自然语言沟通的方法，特别是在不需要大量人类演示数据的情况下。研究以《Among Us》游戏为背景，旨在训练智能体通过自然语言讨论来推断游戏中的关键信息（如谁是冒充者），从而提高团队合作和竞争的表现。以下是论文的主要内容和贡献：</p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><ul>
<li>在多智能体环境中，智能体之间的沟通对于实现共同目标至关重要，尤其是在信息不完全可观测的场景中。</li>
<li>《Among Us》作为一个典型的社交推理游戏，要求玩家通过沟通来推断冒充者的身份，这为研究智能体之间的沟通提供了一个理想的实验平台。</li>
<li>现有的方法要么依赖于大量人类演示数据，要么在沟通能力上存在局限性。本文提出了一种新的方法，通过利用智能体的目标（如推断冒充者身份）作为密集奖励信号，来指导智能体的沟通行为。</li>
</ul>
<h3 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h3><ul>
<li><strong>问题分解</strong>：将沟通问题分解为“倾听”和“说话”两个部分，分别进行优化。</li>
<li><strong>倾听：冒充者预测</strong>：通过监督学习任务，训练智能体根据环境观察和之前的讨论来预测冒充者的身份。这为智能体提供了一个密集的奖励信号，帮助它们更好地理解其他智能体的信息。</li>
<li><strong>说话：强化讨论学习</strong>：通过强化学习，训练智能体生成有用的讨论信息。智能体的奖励基于其发言对其他智能体关于冒充者身份信念的影响。</li>
<li><strong>多智能体强化学习（MARL）</strong>：使用MARL来训练智能体，使其能够在复杂的社交环境中进行有效的沟通和决策。</li>
<li><strong>迭代自我对弈</strong>：通过迭代自我对弈，让智能体与前一轮训练出的对手进行对弈，从而不断适应和改进自己的策略。</li>
</ul>
<h3 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h3><ul>
<li><strong>不同训练算法的性能比较</strong>：比较了使用不同训练算法的智能体在《Among Us》游戏中的表现，包括基础语言模型、仅使用强化学习、仅使用倾听训练、强化学习加倾听、以及强化学习加倾听加说话。</li>
<li><strong>环境配置的鲁棒性测试</strong>：改变了游戏环境的多个参数，包括环境形状、任务数量和玩家数量，以测试智能体在不同配置下的鲁棒性。</li>
<li><strong>对抗不同策略的冒充者的鲁棒性测试</strong>：进行了多轮迭代自我对弈实验，评估智能体在对抗不同策略的冒充者时的鲁棒性。</li>
<li><strong>讨论阶段的新兴行为分析</strong>：分析了智能体在讨论阶段的新兴行为，如直接指责嫌疑人和提供证据。</li>
</ul>
<h3 id="关键结论"><a href="#关键结论" class="headerlink" title="关键结论"></a>关键结论</h3><ul>
<li><strong>性能提升</strong>：使用强化学习加倾听加说话（RL + L + S）算法训练的智能体在游戏中的表现最佳，其胜率是仅使用强化学习的智能体的两倍，且显著高于基础语言模型。</li>
<li><strong>鲁棒性</strong>：经过训练的智能体在不同环境配置下表现出良好的鲁棒性，并且在对抗性训练的冒充者面前也能保持较高的胜率。</li>
<li><strong>新兴行为</strong>：智能体在讨论阶段能够生成与人类玩家相似的讨论内容，如直接指责嫌疑人和提供证据，这表明智能体能够有效地利用讨论阶段来协调团队决策。</li>
<li><strong>失败模式分析</strong>：论文还记录了在实验过程中遇到的一些失败模式，并探讨了如何通过调整训练算法来解决这些问题，如语言退化、社交规范退化和动作标记滥用。</li>
</ul>
<h3 id="研究贡献"><a href="#研究贡献" class="headerlink" title="研究贡献"></a>研究贡献</h3><ul>
<li>提出了一种新的方法，可以在没有大量人类演示数据的情况下训练智能体进行有效的自然语言沟通。</li>
<li>展示了通过利用智能体的目标作为密集奖励信号，可以显著提高智能体的沟通能力和团队合作表现。</li>
<li>通过实验验证了所提出方法的有效性，并分析了智能体在复杂社交环境中的新兴行为。</li>
<li>提供了对智能体沟通能力的深入理解，并为未来的研究提供了新的方向，如更复杂的游戏环境、更高级的语言模型、更复杂的沟通策略、真实世界的任务应用、理论和算法改进、伦理和社会影响等。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">QianLi Yang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://yangqianli92.github.io/">https://yangqianli92.github.io/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">QianLi Yang</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/Paper/">Paper</a><a class="post-meta__tags" href="/tags/werewolf/">werewolf</a></div><div class="post-share"><div class="social-share" data-image="/images/head.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/06/16/%E7%8B%BC%E4%BA%BA%E6%9D%80/" title="基于大语言模型（LLM）驱动的AI狼人杀综述"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">基于大语言模型（LLM）驱动的AI狼人杀综述</div></div><div class="info-2"><div class="info-item-1">基于大语言模型（LLM）驱动的AI狼人杀综述1. 引言狼人杀（Werewolf）是一款经典的社会推理游戏，其核心特点在于隐藏角色、信息不完全以及复杂的社会互动 。玩家必须仅凭口头交流和观察到的行为来推断他人的真实身份，这要求参与者具备战略性决策、说服能力和欺骗能力 。游戏通常在夜晚和白天交替进行，始于夜晚阶段。在夜晚，狼人秘密选择受害者进行淘汰，而预言家和女巫等特殊角色则执行各自的独特能力 。白天阶段，幸存玩家进行公开讨论，随后集体投票淘汰一名被怀疑的狼人 。游戏胜负取决于狼人是否全部被淘汰（村民阵营获胜），或狼人数量达到或超过剩余村民数量（狼人阵营获胜）。  狼人杀作为人工智能（AI）研究的测试平台，其价值非凡。它提供了一个理想的框架，用于评估AI在复杂社会环境中的能力，因为它天然依赖于自然语言沟通、战略性推理、欺骗检测和动态协作 。与国际象棋或围棋等完全信息博弈不同，狼人杀的信息不完全性和自由形式的语言空间引入了显著的复杂性，并拓展了战略可能性，使其成为衡量先进AI能力的独特且具有挑战性的基准 。 传统AI在完全信息博弈（如国际象棋和围棋）中取得了显著成功...</div></div></div></a><a class="pagination-related" href="/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A05/" title="LLM论文阅读-狼人杀系列5 Werewolf Arena"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">LLM论文阅读-狼人杀系列5 Werewolf Arena</div></div><div class="info-2"><div class="info-item-1">Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction Authors: Suma Bailis, Jane Friedhoff, Feiyang Chen This paper introduces Werewolf Arena, a novel framework for evaluating large language models (LLMs) through the lens of the classic social deduction game, Werewolf. In Werewolf Arena, LLMs compete against each other, navigating the game’s complex dynamics of deception, deduction, and persuasion. The framework introduces a dynamic turn-taking system based on bidding,...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/06/10/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A02/" title="LLM论文阅读-狼人杀系列 探索LLM在社交类游戏的应用"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-10</div><div class="info-item-2">LLM论文阅读-狼人杀系列 探索LLM在社交类游戏的应用</div></div><div class="info-2"><div class="info-item-1">Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf  Authors: Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu Communication games, which we refer to as incomplete information games that heavily depend on natural language communication, hold significant research value in fields such as economics, social science, and artificial intelligence. In this work, we explore the problem of how to engage large language models (LLMs) in...</div></div></div></a><a class="pagination-related" href="/2025/06/03/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A01/" title="LLM论文阅读-狼人杀系列 强化学习智能体在狼人杀中的策略玩法"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-03</div><div class="info-item-2">LLM论文阅读-狼人杀系列 强化学习智能体在狼人杀中的策略玩法</div></div><div class="info-2"><div class="info-item-1">#1 Language Agents with Reinforcement Learning for Strategic Play in the Werewolf GameAuthors: Zelai Xu, Chao Yu, Fei Fang, Yu Wang, Yi Wu Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model’s training data and results in suboptimal performance. To develop strategic language agents,...</div></div></div></a><a class="pagination-related" href="/2025/06/12/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A03/" title="LLM论文阅读-狼人杀系列 Werewolf 一个直观的游戏框架，配备文本转语音功能以提升用户参与度"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-12</div><div class="info-item-2">LLM论文阅读-狼人杀系列 Werewolf 一个直观的游戏框架，配备文本转语音功能以提升用户参与度</div></div><div class="info-2"><div class="info-item-1">Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement  Authors: Qihui Fan, Enfu Nan, Wenbo Li, Lei Lu, Pu Zhao, Yanzhi Wang The growing popularity of social deduction game systems for both business applications and AI research has greatly benefited from the rapid advancements in Large Language Models (LLMs), which now demonstrate stronger reasoning and persuasion capabilities. Especially with the raise of DeepSeek R1 and V3 models, LLMs should enable a more...</div></div></div></a><a class="pagination-related" href="/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A05/" title="LLM论文阅读-狼人杀系列5 Werewolf Arena"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-16</div><div class="info-item-2">LLM论文阅读-狼人杀系列5 Werewolf Arena</div></div><div class="info-2"><div class="info-item-1">Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction Authors: Suma Bailis, Jane Friedhoff, Feiyang Chen This paper introduces Werewolf Arena, a novel framework for evaluating large language models (LLMs) through the lens of the classic social deduction game, Werewolf. In Werewolf Arena, LLMs compete against each other, navigating the game’s complex dynamics of deception, deduction, and persuasion. The framework introduces a dynamic turn-taking system based on bidding,...</div></div></div></a><a class="pagination-related" href="/2025/06/16/%E7%8B%BC%E4%BA%BA%E6%9D%80/" title="基于大语言模型（LLM）驱动的AI狼人杀综述"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-16</div><div class="info-item-2">基于大语言模型（LLM）驱动的AI狼人杀综述</div></div><div class="info-2"><div class="info-item-1">基于大语言模型（LLM）驱动的AI狼人杀综述1. 引言狼人杀（Werewolf）是一款经典的社会推理游戏，其核心特点在于隐藏角色、信息不完全以及复杂的社会互动 。玩家必须仅凭口头交流和观察到的行为来推断他人的真实身份，这要求参与者具备战略性决策、说服能力和欺骗能力 。游戏通常在夜晚和白天交替进行，始于夜晚阶段。在夜晚，狼人秘密选择受害者进行淘汰，而预言家和女巫等特殊角色则执行各自的独特能力 。白天阶段，幸存玩家进行公开讨论，随后集体投票淘汰一名被怀疑的狼人 。游戏胜负取决于狼人是否全部被淘汰（村民阵营获胜），或狼人数量达到或超过剩余村民数量（狼人阵营获胜）。  狼人杀作为人工智能（AI）研究的测试平台，其价值非凡。它提供了一个理想的框架，用于评估AI在复杂社会环境中的能力，因为它天然依赖于自然语言沟通、战略性推理、欺骗检测和动态协作 。与国际象棋或围棋等完全信息博弈不同，狼人杀的信息不完全性和自由形式的语言空间引入了显著的复杂性，并拓展了战略可能性，使其成为衡量先进AI能力的独特且具有挑战性的基准 。 传统AI在完全信息博弈（如国际象棋和围棋）中取得了显著成功...</div></div></div></a><a class="pagination-related" href="/2025/06/03/MCP-RAG/" title="LLM论文阅读:RAG-MCP：通过检索增强生成技术缓解大型语言模型工具选择中的提示膨胀问题"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-03</div><div class="info-item-2">LLM论文阅读:RAG-MCP：通过检索增强生成技术缓解大型语言模型工具选择中的提示膨胀问题</div></div><div class="info-2"><div class="info-item-1">RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation [PDF5] [Copy] [Kimi19] [REL]Authors: Tiantian Gan, Qiyao Sun Large language models (LLMs) struggle to effectively utilize a growing number of external tools, such as those defined by the Model Context Protocol (MCP)\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We introduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes this challenge by offloading tool...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/images/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">QianLi Yang</div><div class="author-info-description">不定时更新程序员成长之路~</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">17</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/YangQianli92" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:chainllie92@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/Serein_Young" target="_blank" title="Wechat"><i class="fab fa-weixin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E9%97%AE%E9%A2%98%E5%88%86%E8%A7%A3"><span class="toc-number">1.</span> <span class="toc-text">1. 问题分解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%88%A9%E7%94%A8%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%9B%AE%E6%A0%87%E4%BD%9C%E4%B8%BA%E5%A5%96%E5%8A%B1%E4%BF%A1%E5%8F%B7"><span class="toc-number">2.</span> <span class="toc-text">2. 利用智能体的目标作为奖励信号</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%80%BE%E5%90%AC%EF%BC%9A%E5%86%92%E5%85%85%E8%80%85%E9%A2%84%E6%B5%8B%EF%BC%88Imposter-Prediction%EF%BC%89"><span class="toc-number">2.1.</span> <span class="toc-text">（1）倾听：冒充者预测（Imposter Prediction）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E8%AF%B4%E8%AF%9D%EF%BC%9A%E5%BC%BA%E5%8C%96%E8%AE%A8%E8%AE%BA%E5%AD%A6%E4%B9%A0%EF%BC%88Reinforced-Discussion-Learning%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">（2）说话：强化讨论学习（Reinforced Discussion Learning）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88Multi-Agent-Reinforcement-Learning-MARL%EF%BC%89"><span class="toc-number">3.</span> <span class="toc-text">3. 多智能体强化学习（Multi-Agent Reinforcement Learning, MARL）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E8%BF%AD%E4%BB%A3%E8%87%AA%E6%88%91%E5%AF%B9%E5%BC%88%EF%BC%88Iterated-Self-Play%EF%BC%89"><span class="toc-number">4.</span> <span class="toc-text">4. 迭代自我对弈（Iterated Self-Play）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E4%BD%BF%E7%94%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLMs%EF%BC%89"><span class="toc-number">5.</span> <span class="toc-text">5. 使用大型语言模型（LLMs）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E5%AE%9E%E9%AA%8C%E9%AA%8C%E8%AF%81"><span class="toc-number">6.</span> <span class="toc-text">6. 实验验证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%B8%8D%E5%90%8C%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83"><span class="toc-number">7.</span> <span class="toc-text">1. 不同训练算法的性能比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E7%9A%84%E9%B2%81%E6%A3%92%E6%80%A7%E6%B5%8B%E8%AF%95"><span class="toc-number">8.</span> <span class="toc-text">2. 环境配置的鲁棒性测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%AF%B9%E6%8A%97%E4%B8%8D%E5%90%8C%E7%AD%96%E7%95%A5%E7%9A%84%E5%86%92%E5%85%85%E8%80%85%E7%9A%84%E9%B2%81%E6%A3%92%E6%80%A7%E6%B5%8B%E8%AF%95"><span class="toc-number">9.</span> <span class="toc-text">3. 对抗不同策略的冒充者的鲁棒性测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E8%AE%A8%E8%AE%BA%E9%98%B6%E6%AE%B5%E7%9A%84%E6%96%B0%E5%85%B4%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90"><span class="toc-number">10.</span> <span class="toc-text">4. 讨论阶段的新兴行为分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%A4%B1%E8%B4%A5%E6%A8%A1%E5%BC%8F%E5%88%86%E6%9E%90"><span class="toc-number">11.</span> <span class="toc-text">5. 失败模式分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">12.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%9B%B4%E5%A4%8D%E6%9D%82%E7%9A%84%E6%B8%B8%E6%88%8F%E7%8E%AF%E5%A2%83"><span class="toc-number">13.</span> <span class="toc-text">1. 更复杂的游戏环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%9B%B4%E9%AB%98%E7%BA%A7%E7%9A%84%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">14.</span> <span class="toc-text">2. 更高级的语言模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%9B%B4%E5%A4%8D%E6%9D%82%E7%9A%84%E6%B2%9F%E9%80%9A%E7%AD%96%E7%95%A5"><span class="toc-number">15.</span> <span class="toc-text">3. 更复杂的沟通策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E7%9C%9F%E5%AE%9E%E4%B8%96%E7%95%8C%E7%9A%84%E4%BB%BB%E5%8A%A1"><span class="toc-number">16.</span> <span class="toc-text">4. 真实世界的任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E7%90%86%E8%AE%BA%E5%92%8C%E7%AE%97%E6%B3%95%E6%94%B9%E8%BF%9B"><span class="toc-number">17.</span> <span class="toc-text">5. 理论和算法改进</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E4%BC%A6%E7%90%86%E5%92%8C%E7%A4%BE%E4%BC%9A%E5%BD%B1%E5%93%8D"><span class="toc-number">18.</span> <span class="toc-text">6. 伦理和社会影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E8%B7%A8%E6%96%87%E5%8C%96%E6%B2%9F%E9%80%9A"><span class="toc-number">19.</span> <span class="toc-text">7. 跨文化沟通</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86%E5%92%8C%E7%9F%A5%E8%AF%86%E7%A7%AF%E7%B4%AF"><span class="toc-number">20.</span> <span class="toc-text">8. 长期记忆和知识积累</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF"><span class="toc-number">21.</span> <span class="toc-text">研究背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95"><span class="toc-number">22.</span> <span class="toc-text">研究方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1"><span class="toc-number">23.</span> <span class="toc-text">实验设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E7%BB%93%E8%AE%BA"><span class="toc-number">24.</span> <span class="toc-text">关键结论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A0%94%E7%A9%B6%E8%B4%A1%E7%8C%AE"><span class="toc-number">25.</span> <span class="toc-text">研究贡献</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/18/%E6%A2%A6%E5%B9%BB%E8%A5%BF%E6%B8%B8%E4%BD%93%E9%AA%8C%E6%8A%A5%E5%91%8A/" title="无标题">无标题</a><time datetime="2025-06-18T13:53:18.873Z" title="发表于 2025-06-18 21:53:18">2025-06-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/18/%E6%A2%A6%E5%B9%BB%E8%A5%BF%E6%B8%B8/" title="AI赋能《梦幻西游手游》内挂：利用大语言模型与智能体实现玩家托管自动化">AI赋能《梦幻西游手游》内挂：利用大语言模型与智能体实现玩家托管自动化</a><time datetime="2025-06-18T12:44:48.000Z" title="发表于 2025-06-18 20:44:48">2025-06-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/16/%E7%8B%BC%E4%BA%BA%E6%9D%80/" title="基于大语言模型（LLM）驱动的AI狼人杀综述">基于大语言模型（LLM）驱动的AI狼人杀综述</a><time datetime="2025-06-16T09:31:08.000Z" title="发表于 2025-06-16 17:31:08">2025-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A04/" title="LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型">LLM论文阅读-狼人杀系列4 利用多智能体强化学习训练社交推理语言模型</a><time datetime="2025-06-16T04:46:48.000Z" title="发表于 2025-06-16 12:46:48">2025-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/16/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E7%8B%BC%E4%BA%BA%E6%9D%80%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A05/" title="LLM论文阅读-狼人杀系列5 Werewolf Arena">LLM论文阅读-狼人杀系列5 Werewolf Arena</a><time datetime="2025-06-16T04:46:48.000Z" title="发表于 2025-06-16 12:46:48">2025-06-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By QianLi Yang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>